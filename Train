from __future__ import division
from __future__ import print_function


# Set random seed
seed = 123
np.random.seed(seed)
tf.set_random_seed(seed)

# Settings
flags = tf.app.flags
FLAGS = flags.FLAGS

#flags.DEFINE_string('dataset', 'pubmed', 'Dataset string.')  # 'cora', 'citeseer', 'pubmed'
flags.DEFINE_string('model', 'gcn_cheby', 'Model string.')  # 'gcn', 'gcn_cheby', 'dense'
flags.DEFINE_float('learning_rate', 0.002, 'Initial learning rate.')
flags.DEFINE_integer('epochs', 800, 'Number of epochs to train.')
flags.DEFINE_integer('hidden1', 128, 'Number of units in hidden layer 1.')
flags.DEFINE_integer('hidden2', 64, 'Number of units in hidden layer 2.')
flags.DEFINE_integer('hidden3', 32, 'Number of units in hidden layer 3.')
flags.DEFINE_float('dropout', 0.0, 'Dropout rate (1 - keep probability).')
flags.DEFINE_float('weight_decay', 0.0, 'Weight for L2 loss on embedding matrix.')
flags.DEFINE_integer('early_stopping', 50, 'Tolerance for early stopping (# of epochs).')
flags.DEFINE_integer('max_degree', 1, 'Maximum Chebyshev polynomial degree.')


rank0 = 10000
rank1 = 10000
rank2 = 10000
batch_size = 20000


# Some preprocessing
features = nontuple_preprocess_features(features)
train_features = features[train_index]

val_features = sparse_to_tuple(features[val_index])
test_features = sparse_to_tuple(features[test_index])


features = sparse_to_tuple(features)


if FLAGS.model == 'gcn':
    #support = [preprocess_adj(adj)]
    
    normADJ_train = nontuple_preprocess_adj(adj_train)
    normADJ = nontuple_preprocess_adj(adj)
    
    num_supports = 1
    model_func = GCN
    num_layers = 2
    
elif FLAGS.model == 'gcn_cheby':
    
    normADJ_train = nontuple_preprocess_adj(adj_train)
    normADJ = nontuple_preprocess_adj(adj)
    normcheby_train = chebyshev_polynomials(adj_train, FLAGS.max_degree)
    normcheby = chebyshev_polynomials(adj, FLAGS.max_degree)
    num_supports = 1 + FLAGS.max_degree
    model_func = GCN
    num_layers = 2
    
#elif FLAGS.model == 'dense':
#    support = [preprocess_adj(adj)]  # Not used
#    num_supports = 1
#   model_func = MLP
else:
    raise ValueError('Invalid argument for model: ' + str(FLAGS.model))
    
    
    
 
# Define placeholders
placeholders = {
    'support': [[tf.sparse_placeholder(tf.float32) for ii in range(num_supports)] for _ in range(num_layers)],
    'features': tf.sparse_placeholder(tf.float32, shape=(None, features[2][1]) ),
    'labels': tf.placeholder(tf.float32, shape=(None, y_train.shape[1])),
    'labels_mask': tf.placeholder(tf.int32),
    'dropout': tf.placeholder_with_default(0., shape=()),
    'num_features_nonzero': tf.placeholder(tf.int32)  # helper variable for sparse dropout
}

# Create model
model = model_func(placeholders, input_dim=features[2][1], logging=True)

# Initialize session
sess = tf.Session()

# Define model evaluation function
def evaluate(features, support, labels, mask, placeholders):
    t_test = time.time()
    feed_dict_val = construct_feed_dict(features, support, labels, mask, placeholders)
    outs_val = sess.run([model.loss, model.accuracy], feed_dict=feed_dict_val)
    return outs_val[0], outs_val[1], (time.time() - t_test)


# Init variables
sess.run(tf.global_variables_initializer())

cost_val = []
#p0 = column_prop(normADJ_train)

valSupport = [[sparse_to_tuple(normcheby[0]), sparse_to_tuple(normcheby[1])], 
              #[sparse_to_tuple(normcheby[0]), sparse_to_tuple(normcheby[1])],
              [sparse_to_tuple(normcheby[0][val_index, :]), sparse_to_tuple(normcheby[1][val_index, :])]]
testSupport = [[sparse_to_tuple(normcheby[0]), sparse_to_tuple(normcheby[1])], 
              #[sparse_to_tuple(normcheby[0]), sparse_to_tuple(normcheby[1])],
              [sparse_to_tuple(normcheby[0][test_index, :]), sparse_to_tuple(normcheby[1][test_index, :])]]

# Train model

for epoch in range(FLAGS.epochs):

    t = time.time()
    
    for batch in iterate_minibatches_listinputs([normADJ_train, normcheby_train[0], normcheby_train[1], y_train, train_mask], batchsize=batch_size, shuffle=True):
        
        
        t_batch = time.time()
        
        [normADJ_batch, normcheby_batch0, normcheby_batch1, y_train_batch, train_mask_batch] = batch
        if sum(train_mask_batch) < 1:
            continue
        
        ##support matrix of layer1 (output->...)
        p2 = column_prop(normADJ_batch)
        q2 = np.random.choice(np.arange(numNode_train), rank2, p=p2)  # top layer
        support2 = [sparse_to_tuple(normcheby_batch0[:, q2].dot(sp.diags(1.0 / (p2[q2] * rank2)))), 
                   sparse_to_tuple(normcheby_batch1[:, q2].dot(sp.diags(1.0 / (p2[q2] * rank2))))]

        ##support matrix of layer2
        p1 = column_prop(normADJ_train[q2, :])
        q1 = np.random.choice(np.arange(numNode_train), rank1, p=p1)
        support1 = [sparse_to_tuple(normcheby_train[0][q2, :][:, q1]),
                   sparse_to_tuple(normcheby_train[1][q2, :][:, q1])]
        
        ##support matrix of layer3
        #p0 = column_prop(normADJ_train[q1, :])
        #q0 = np.random.choice(np.arange(numNode_train), rank0, p=p0)
        #support0 = [sparse_to_tuple(normcheby_train[0][q1, :][:, q0]),
        #           sparse_to_tuple(normcheby_train[1][q1, :][:, q0])]
        
        features_inputs = sp.diags(1.0 / (p1[q1] * rank1)).dot(train_features[q1, :])  # selected nodes for approximation
        features_inputs = sparse_to_tuple(features_inputs)

        # Construct feed dictionary
        feed_dict = construct_feed_dict(features_inputs, [  support1, support2], y_train_batch, train_mask_batch, placeholders)
        feed_dict.update({placeholders['dropout']: FLAGS.dropout})
        #print('feed finished')
        # Training step
        outs = sess.run([model.opt_op, model.loss, model.accuracy], feed_dict=feed_dict)
        
        #print("batch time: ","{:.5f}".format(time.time()-t_batch))
    
    # Validation
    
    cost, acc, duration = evaluate(features, valSupport, y_val, val_mask, placeholders)
    cost_val.append(cost)
    
    # Print results
    print("Epoch:", '%04d' % (epoch + 1), "train_loss=", "{:.5f}".format(outs[1]),
          "train_acc=", "{:.5f}".format(outs[2]), "val_loss=", "{:.5f}".format(cost),
          "val_acc=", "{:.5f}".format(acc), "time=", "{:.5f}".format(time.time() - t))

    #if epoch > FLAGS.early_stopping and cost_val[-1] > np.mean(cost_val[-(FLAGS.early_stopping+1):-1]):
    #    print("Early stopping...")
    #    break

print("Optimization Finished!")

# Testing
test_cost, test_acc, test_duration = evaluate(features, testSupport, y_test, test_mask, placeholders)
print("Test set results:", "cost=", "{:.5f}".format(test_cost),
      "accuracy=", "{:.5f}".format(test_acc), "time=", "{:.5f}".format(test_duration))




# predict

feed_dict_test = construct_feed_dict(features, valSupport, y_val, val_mask, placeholders)
out = sess.run(model.predict, feed_dict=feed_dict_test)
print(out)

print(label[100000])
print(out[0])
